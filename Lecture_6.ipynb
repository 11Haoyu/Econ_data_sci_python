{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8f817c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Optimization\n",
    "\n",
    "Zhentao Shi\n",
    "\n",
    "<!-- code is tested on SCRP -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856666bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Optimization \n",
    "\n",
    "* Econometrics curriculum does not pay enough attention to numerical optimization\n",
    "* Most estimators solve optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2cbc9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Operational research\n",
    "* Understand the essence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874e9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### User Cases\n",
    "\n",
    "* Maximum likelihood estimation\n",
    "* Discrete / mixed data type\n",
    "* Machine learning / regularization\n",
    "* Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fdf34",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Big data stochastic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199d2a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* Generic optimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\theta \\in \\Theta } f(\\theta) \\,\\, \\mathrm{ s.t. }  g(\\theta) = 0, h(\\theta) \\leq 0,\n",
    "$$\n",
    "\n",
    "* $f(\\cdot)\\in \\mathbb{R}$: criterion function\n",
    "* $g(\\theta) = 0$: a vector of equality constraints\n",
    "* $h(\\theta)\\leq 0$: a vector of inequality constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081bf3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* unconstrained\n",
    "* constrained\n",
    "* Lagrangian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b4b97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convenience vs. Efficiency\n",
    "\n",
    "* Convenience: readability of the mathematical expressions and the code\n",
    "* Efficiency:  computing speed\n",
    "\n",
    "* Put convenience as priority at the trial-and-error stage, \n",
    "* Improve efficiency when necessary at a later stage for full-scale execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae8ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "* Many optimization algorithms\n",
    "* Variants of a few fundamental principles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d36f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's Method\n",
    "\n",
    "* Essential idea for optimizing a twice-differentiable objective function\n",
    "* Necessary condition: the first-order condition\n",
    "\n",
    "$$\n",
    "s(\\theta) = \\partial f(\\theta) / \\partial \\theta = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630674fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define functions\n",
    "def f(x):\n",
    "    return 0.1 * (x - 5) ** 2 + np.cos(x)\n",
    "\n",
    "def s(x):\n",
    "    return 0.2 * (x - 5) - np.sin(x)\n",
    "\n",
    "def h(x):\n",
    "    return 0.2 - np.cos(x)\n",
    "\n",
    "# create x array\n",
    "x_base = np.arange(0.1, 10, 0.1)\n",
    "\n",
    "# create plots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(6, 8))\n",
    "\n",
    "axs[0].plot(x_base, f(x_base), linewidth=2)\n",
    "axs[0].set_ylabel('f')\n",
    "\n",
    "axs[1].plot(x_base, s(x_base))\n",
    "axs[1].axhline(y=0, linestyle='--')\n",
    "axs[1].set_ylabel('score')\n",
    "\n",
    "axs[2].plot(x_base, h(x_base))\n",
    "axs[2].axhline(y=0, linestyle='--')\n",
    "axs[2].set_ylabel('Hessian')\n",
    "axs[2].set_xlabel('x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e511e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteartion\n",
    "\n",
    "* Initial trial value $\\theta_0$, \n",
    "* If $s(\\theta_0) \\neq 0$, updated by\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} -  \\left( H(\\theta_t)  \\right)^{-1}  s(\\theta_t)\n",
    "$$\n",
    "\n",
    "for the index of iteration $t=0,1,\\cdots$\n",
    "* $H(\\theta) = \\frac{ \\partial s(\\theta )}{ \\partial \\theta}$ is the Hessian. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "805cb59d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mechanism\n",
    "\n",
    "* Taylor expansion\n",
    "at $\\theta_t$ round  $\\theta_{\\star}$, a root of $s(\\cdot)$. Because $\\theta_{ \\star }$  is a root,\n",
    "\n",
    "$$\n",
    "0 = s(\\theta_{\\star}) = s(\\theta_t) + H(\\theta_t) (\\theta_{\\star} - \\theta_t) + O( (\\theta_{\\star} - \\theta_t)^2 ).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1daa97b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Update\n",
    "\n",
    "* Ignore the high-order term and rearrange,\n",
    "\n",
    "$$\n",
    "\\theta_{\\star} = \\theta_{t} -  \\left( H(\\theta_t)  \\right)^{-1}  s(\\theta_t)\n",
    "$$ \n",
    "\n",
    "* iteration formula by replacing $\\theta_{\\star}$ with the updated $\\theta_{t+1}$. \n",
    "* Iterate until $|\\theta_{t+1} -\\theta_{t}| < \\epsilon$ (absolute criterion) and/or\n",
    "$|\\theta_{t+1} -\\theta_{t}|/|\\theta_{t}| < \\epsilon$ (relative criterion), \n",
    "* $\\epsilon$ is a small positive number chosen as a tolerance level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ce8c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def Newton(x):\n",
    "    return x - s(x) / h(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413f934",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_init = 1  # can experiment with various initial values\n",
    "\n",
    "gap = 1\n",
    "epsilon = 0.000001  # tolerance\n",
    "while gap > epsilon:\n",
    "    x_new = Newton(x_init)\n",
    "    print(x_new)\n",
    "    gap = abs(x_init - x_new)\n",
    "    x_init = x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf0c57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features of Newton's Method\n",
    "\n",
    "\n",
    "* It seeks the solution to $s(\\theta) = 0$. \n",
    "* The first-order condition is necessary but not sufficient\n",
    "* Verify the second-order condition\n",
    "* Compare the value of multiple minima for global minimum\n",
    "\n",
    "* It requires gradient $s(\\theta)$ and the Hessian $H(\\theta)$.\n",
    "* It numerically converges at quadratic rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1220f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quasi-Newton Method\n",
    "\n",
    "* Most well-known quasi-Newton algorithm is [BFGS](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "* It avoids explicit calculation of Hessian\n",
    "* It starts from an initial (inverse) Hessian\n",
    "* Updates Hessian by an explicit formula via quadratic approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b848391",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Derivative-Free Method\n",
    "\n",
    "* [Nelder-Mead](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)\n",
    "* Simplex method\n",
    "* Search a local minimum \n",
    "  * reflection\n",
    "  * expansion\n",
    "  * contraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ed7d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Package\n",
    "\n",
    "* [R Optimization Task View](http://cran.r-project.org/web/views/Optimization.html) \n",
    "* Package [`optimx`](http://cran.r-project.org/web/packages/optimx/index.html) ([Nash, 2014](https://www.jstatsoft.org/article/view/v060i02))\n",
    "  * Unified interface for various widely-used algorithms. \n",
    "  * Facilitates comparison among optimization algorithms\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6362e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Use `optimx` to solve pseudo Poisson maximum likelihood estimation (PPML)\n",
    "* Popular estimator for cross-country bilateral trade\n",
    "* Conditional mean model\n",
    "\n",
    "$$\n",
    "E[y_i | x_i] = \\exp( x_i' \\beta),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abe9de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poisson MLE\n",
    "\n",
    "If $Z \\sim Poisson(\\lambda)$, then \n",
    "\n",
    "$$\n",
    "\\Pr(Z = k) = \\frac{\\mathrm{e}^{-\\lambda} \\lambda^k}{k!}, \\mathrm{ for }\\, \\, k=0,1,2,\\ldots,\n",
    "$$\n",
    "\n",
    "and the log-likelihood\n",
    "\n",
    "$$\n",
    "\\log \\Pr(Y = y | x) =  -\\exp(x'\\beta) + y\\cdot x'\\beta - \\log k!\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3925c61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Log-likelihood function of the sample\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\log \\Pr( \\mathbf{y} | \\mathbf{x};\\beta ) =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta) + \\sum_{i=1}^n y_i x_i'\\beta.\n",
    "$$\n",
    "\n",
    "* gradient\n",
    "\n",
    "$$\n",
    "s(\\beta) =\\frac{\\partial \\ell(\\beta)}{\\partial \\beta} =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta)x_i + \\sum_{i=1}^n y_i x_i.\n",
    "$$\n",
    "\n",
    "* Hessian\n",
    "\n",
    "$$\n",
    "H(\\beta) = \\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta \\partial \\beta'} =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta)x_i x_i'\n",
    "$$\n",
    "\n",
    "is negative definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f29d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $\\ell(\\beta)$ is strictly concave in $\\beta$.\n",
    "\n",
    "* Default optimization is minimization\n",
    "* Use *negative* log-likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecb096",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def poisson_loglik(b):\n",
    "    b = np.ravel(b)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.sum(-lambda_ + y * np.log(lambda_))\n",
    "    return ell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15921bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Write the criterion as a function of the parameter to be optimized \n",
    "* Data can be fed inside or outside of the function.\n",
    "  * If the data is provided as additional arguments, these arguments must be explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4501a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "## prepare the data\n",
    "data = sm.datasets.get_rdataset('RecreationDemand', 'AER').data\n",
    "y = data['trips']\n",
    "X = data[['income']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "## estimation\n",
    "b_init = [0, 1]  # initial value\n",
    "b_hat_bfgs = minimize(poisson_loglik, b_init, method='BFGS', options={'gtol': 1e-7, 'disp': True})\n",
    "b_hat_nm = minimize(poisson_loglik, b_init, method='Nelder-Mead', options={'xtol': 1e-7, 'disp': True})\n",
    "\n",
    "print(f\"BFGS: {b_hat_bfgs}\")\n",
    "print(f\"Nelder-Mead: {b_hat_nm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed425ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative Formulation\n",
    "\n",
    "* Nonlinear least squares (NLS) is also valid in theory.\n",
    "* NLS minimizes\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - \\exp(x_i \\beta))^2\n",
    "$$\n",
    "\n",
    "* Why PPML is preferred? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678ce6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* PPML's optimization for the linear index is globally convex.\n",
    "* Numerical optimization of PPML is easier and more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502f7e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats\n",
    "\n",
    "* No algorithm suits all problems. \n",
    "* Simulation is helpful to check the accuracy of optimization\n",
    "* Contour plot helps visualize the function surface/manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464c1d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e534db3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "## generate contour plot\n",
    "x_grid = np.arange(0, 1.4, 0.02)\n",
    "x_length = len(x_grid)\n",
    "y_grid = np.arange(-0.5, 0.2, 0.01)\n",
    "y_length = len(y_grid)\n",
    "\n",
    "z_contour = np.zeros((x_length, y_length))\n",
    "\n",
    "for i in range(x_length):\n",
    "    for j in range(y_length):\n",
    "        z_contour[i, j] = poisson_loglik([x_grid[i], y_grid[j]])\n",
    "\n",
    "plt.contour(x_grid, y_grid, z_contour, 20)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Contour Plot')\n",
    "plt.show()\n",
    "\n",
    "## generate filled contour plot\n",
    "plt.contourf(x_grid, y_grid, z_contour.T, 20)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Filled Contour Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964e0b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLOPT\n",
    "\n",
    "* Third-party standalone solvers \n",
    "* [`NLopt`](http://ab-initio.mit.edu/wiki/index.php/NLopt_Installation)\n",
    "* [extensive list of algorithms](http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms#SLSQP)\n",
    "* Package [`nloptr`](http://cran.r-project.org/web/packages/nloptr/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ec5c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "We first carry out the Nelder-Mead algorithm in NLOPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b14a64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## optimization with Nelder-Mead algorithm\n",
    "from nlopt import nlopt\n",
    "import numpy as np\n",
    "\n",
    "def poisson_loglik_2(params, grad):\n",
    "    b = np.ravel(params)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.sum(-lambda_ + y * np.log(lambda_))\n",
    "    return ell\n",
    "\n",
    "opts = {\n",
    "    'algorithm': nlopt.LN_NELDERMEAD,\n",
    "    'xtol_rel': 1e-7,\n",
    "    'maxeval': 500\n",
    "}\n",
    "\n",
    "res_NM = nlopt.opt(opts['algorithm'], 2)\n",
    "res_NM.set_lower_bounds([-np.inf, 0])\n",
    "res_NM.set_min_objective(poisson_loglik_2)\n",
    "res_NM.set_xtol_rel(opts['xtol_rel'])\n",
    "x0 = b_init\n",
    "res_NM.optimize(x0)\n",
    "print(f\"Nelder-Mead algorithm: {res_NM.last_optimize_result()}\")\n",
    "\n",
    "## optimization with SLSQP algorithm (equivalent to BFGS)\n",
    "opts = {\n",
    "    'algorithm': nlopt.LD_SLSQP,\n",
    "    'xtol_rel': 1e-7\n",
    "}\n",
    "\n",
    "res_SLSQP = nlopt.opt(opts['algorithm'], 2)\n",
    "res_SLSQP.set_lower_bounds([-np.inf, 0])\n",
    "res_SLSQP.set_min_objective(poisson_loglik_2)\n",
    "res_SLSQP.set_xtol_rel(opts['xtol_rel'])\n",
    "x0 = b_init\n",
    "res_SLSQP.optimize(x0)\n",
    "print(f\"SLSQP algorithm: {res_SLSQP.last_optimize_result()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03769c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To invoke BFGS in NLOPT, we must code up the gradient $s(\\beta)$,\n",
    "as in the function `poisson.log.grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e313fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def poisson_loglik_grad(b):\n",
    "    b = np.ravel(b)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.sum(-lambda_ + y * lambda_)\n",
    "    grad = -(X.T @ (y - lambda_))\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed9195",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Compare the analytical gradient with the numerical gradient\n",
    "* Ensure the code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1cf5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "\n",
    "b = np.array([0, 0.5])\n",
    "\n",
    "# check the numerical gradient\n",
    "num_grad = nd.Gradient(poisson_loglik)(b)\n",
    "print(f\"Numerical gradient: {num_grad}\")\n",
    "\n",
    "# check the analytical gradient\n",
    "ana_grad = poisson_loglik_grad(b)\n",
    "print(f\"Analytical gradient: {ana_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_loglik_grad_2(b, grad):\n",
    "    b = np.ravel(b)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.sum(-lambda_ + y * np.log(lambda_))\n",
    "    if grad.size > 0:\n",
    "        grad[:] = -(X.T @ (y - lambda_))\n",
    "    return ell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1a291",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BFGS with Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268d5fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'algorithm': nlopt.LD_LBFGS,\n",
    "    'xtol_rel': 1e-7,\n",
    "}\n",
    "\n",
    "# Set up the optimization object\n",
    "res_BFGS = nlopt.opt(opts['algorithm'], 2)\n",
    "\n",
    "# Set the lower bounds for the parameters\n",
    "res_BFGS.set_lower_bounds([-np.inf, 0])\n",
    "\n",
    "# Set the objective function and tolerance for convergence\n",
    "res_BFGS.set_min_objective(poisson_loglik_grad_2)\n",
    "res_BFGS.set_xtol_rel(opts['xtol_rel'])\n",
    "\n",
    "# Set the initial parameter values and optimize\n",
    "x0 = np.zeros(2)\n",
    "res_BFGS.optimize(x0)\n",
    "\n",
    "# Print the optimization result\n",
    "print(f\"BFGS algorithm with analytical gradient: {res_BFGS.last_optimize_result()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4891ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Optimization\n",
    "\n",
    "* Local minimum is a global minimum.\n",
    "* Particularly important in high-dimensional problems\n",
    "* [Boyd and Vandenberghe (2004)](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)\n",
    "* \"Convex optimization is technology; all other optimizations are arts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7cfa8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the functions\n",
    "f1 = lambda x: x ** 2\n",
    "f2 = lambda x: np.abs(x)\n",
    "f3 = lambda x: np.where(x <= -1, (-x - 1), np.where(x >= 0.5, 0.4 * x - 0.2, 0))\n",
    "\n",
    "# Set up the plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot f1\n",
    "x_base = np.arange(-3, 3, 0.1)\n",
    "axs[0].plot(x_base, f1(x_base), lw=2)\n",
    "axs[0].set_xlabel('differentiable')\n",
    "\n",
    "# Plot f2\n",
    "axs[1].plot(x_base, f2(x_base), lw=2)\n",
    "axs[1].set_xlabel('non-differentiable')\n",
    "\n",
    "# Plot f3\n",
    "axs[2].plot(x_base, f3(x_base), lw=2)\n",
    "axs[2].set_xlabel('multiple minimizers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722accfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da30f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Linear regression model MLE\n",
    "\n",
    "\n",
    "* Normal MLE. The (negative) log-likelihood \n",
    "\n",
    "$$\n",
    "\\ell (\\beta, \\sigma) = \\log \\sigma + \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i' \\beta)^2\n",
    "$$\n",
    "\n",
    "is not convex\n",
    "\n",
    "* Re-parameterize the criterion function by $\\gamma = 1/\\sigma$ and $\\alpha = \\beta / \\sigma$, then\n",
    "\n",
    "$$\n",
    "\\ell (\\alpha, \\gamma) = -\\log \\gamma + \\frac{1}{2}\n",
    "\\sum_{i=1}^n (\\gamma y_i - x_i' \\alpha)^2\n",
    "$$\n",
    "\n",
    "is convex in $\\alpha, \\gamma$\n",
    "\n",
    "* Many MLE estimators in econometric textbooks are convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620629e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [Gao and Shi (2021)](https://link.springer.com/article/10.1007/s10614-020-09995-z) explore the infrastructure in R for convex optimization with two econometric examples. \n",
    "\n",
    "* `CVXR` [(Fu, 2018)](https://arxiv.org/abs/1711.07582) is a convenient convex modeling language \n",
    "  * Proprietary solvers `CLEPX`, `MOSEK`, and `Gurubi`\n",
    "  * Open-source solvers `ECOS` and `SDPT3`. \n",
    "\n",
    "* `MOSEK` offers free academic license\n",
    "* [`Rmosek`](http://rmosek.r-forge.r-project.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351321cb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Example: Relaxed empirical likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1bfa9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Consider a model with a \"true\" parameter $\\beta_0$ satisfying the moment condition $\\mathrm{E}\\left[  h\\left(Z_i, \\beta_0 \\right)\\right] = 0_m$, where $\\left\\{Z_i \\right\\}_{i=1}^n$ is the observed data, $\\beta$\n",
    "is a low dimensional parameter of interest, and  $h$ is an $\\mathbb{R}^{m}$-valued moment function. \n",
    "Empirical likelihood (EL) [[Owen, 1988]](https://www.jstor.org/stable/2336172) [[Qin, 1994]](https://scholar.google.com/scholar_url?url=https://projecteuclid.org/journals/annals-of-statistics/volume-22/issue-1/Empirical-Likelihood-and-General-Estimating-Equations/10.1214/aos/1176325370.pdf&hl=zh-CN&sa=X&ei=jjrrY-iRBueR6rQP_p6hkAQ&scisig=AAGBfm1LBkrfAAtQfwCPvp4R62ge0YKx4A&oi=scholarr) solves\n",
    "\n",
    "$$\n",
    "\\max_{\\beta \\in \\mathcal{B}, \\pi \\Delta_n} \\; \\sum_{i=1}^n \\log \\pi_i \\;\\,\\, \\text{s.t.} \\; \\sum_{i=1}^n \\pi_i h \\left( Z_i, \\beta \\right) = 0_m\n",
    "$$\n",
    "\n",
    "where $\\Delta_{n} = \\left\\{ \\pi\\in\\left[0,1\\right]^{n}:\\sum_{i=1}^{n}\\pi_{i}=1 \\right\\}$ is the $n$-dimensional probability simplex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca99bbb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To handle the high-dimensional case, i.e., $m > n$, [[Shi, 2016]](https://www.sciencedirect.com/science/article/abs/pii/S0304407616301373) proposes the relaxed empirical likelihood (REL),  defined as the solution to \n",
    "\n",
    "$$\n",
    "\\max_{\\beta\\in\\mathcal{B}}\\max_{\\pi\\in\\Delta_{n}^{\\lambda}\\left(\\beta\\right)}\\,\\sum_{i=1}^{n}\\log\\pi_{i}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\Delta_{n}^{\\lambda}\\left(\\beta\\right)=\\left\\{ \\pi_i \\in\\Delta_{n}:\\big|\\sum_{i=1}^{n}\\pi_{i}h_{ij}\\left(\\beta\\right)\\big|\\leq\\lambda,\\:j=1,2,\\cdots,m\\right\\}\n",
    "$$\n",
    "\n",
    "is a relaxed simplex, $\\lambda\\geq0$ is a tuning parameter, $h_{ij}\\left(\\beta\\right)=h_{j}\\left(Z_{i},\\beta\\right)$\n",
    "is the $j$-th component of $h\\left(Z_{i},\\beta\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ca66e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to standard EL, REL's optimization involves an inner loop\n",
    "and an outer loop. The outer loop for $\\beta$ is a general low-dimensional\n",
    "nonlinear optimization, which can be solved by Newton-type methods.\n",
    "With the linear constraints and the logarithm objective, the inner\n",
    "loop is convex in $\\pi=\\left(\\pi_{i}\\right)_{i=1}^{n}$. \n",
    "By introducing auxiliary variable, $t_i$, the logarithm objective can be formulated as a linear objective function $\\sum_{i=1}^n t_i$ and $n$ exponential conic constraints, $\\left(\\pi_{i}, 1, t_{i}\\right) \\in \\mathcal{K}_{\\mathrm{exp}}=\\left\\{\\left(x_{1}, x_{2}, x_{3}\\right): x_{1} \\geq x_{2} \\exp \\left(x_{3} / x_{2}\\right), x_{2}>0\\right\\} \\cup\\left\\{\\left(x_{1}, 0, x_{3}\\right): x_{1} \\geq 0, x_{3} \\leq 0\\right\\}$, $i=1,2,\\cdots,n$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For each $\\beta$, the inner problem can be then formulated as a conic programming problem,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\max _{\\pi, t} \\sum_{i=1}^{n} t_{i}\\\\\n",
    "\\text { s.t. }&\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "-\\lambda \\\\\n",
    "\\vdots \\\\\n",
    "-\\lambda\n",
    "\\end{array}\\right] \\leq\\left[\\begin{array}{cccc}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "h_{11}(\\beta) & h_{21}(\\beta) & \\cdots & h_{n 1}(\\beta) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_{1 m}(\\beta) & h_{2 m}(\\beta) & \\cdots & h_{n m}(\\beta)\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\pi_{1} \\\\\n",
    "\\pi_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\pi_{n}\n",
    "\\end{array}\\right] \\leq\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\lambda \\\\\n",
    "\\vdots \\\\\n",
    "\\lambda\n",
    "\\end{array}\\right]\\\\\n",
    "&\\left(\\pi_{i}, 1, t_{i}\\right) \\in \\mathcal{K}_{\\mathrm{exp}}, 0 \\leq \\pi_{i} \\leq 1, \\text { for each } i=1,2, \\cdots, n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9f0df",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To understand the exponential cone, notice that \n",
    "$\\left(\\pi_{i}, 1, t_{i}\\right) \\in \\mathcal{K}_{\\mathrm{exp}}$ is equivalent to\n",
    "$\\{ \\pi_i \\geq \\exp(t_i): \\pi_i\\geq 0, t_i \\leq 0 \\}$. It implies \n",
    "$t_i \\leq \\log \\pi_i$. Since the problem maximizes $\\sum t_i$, we must have \n",
    "$t_i = \\log \\pi_i$. \n",
    "The constrained optimization is readily solvable in `Rmosek` by translating the mathematical expression into computer code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444735fa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from scipy.sparse import csc_matrix, hstack, vstack\n",
    "\n",
    "def innerloop(b, y, X, Z, tau):\n",
    "    n, p = X.shape\n",
    "    m, k = Z.shape\n",
    "\n",
    "    # Generate moment condition\n",
    "    H = MomentMatrix(y, X, Z, b)\n",
    "\n",
    "    # Initialize the CVXPY problem\n",
    "    beta = cp.Variable(p)\n",
    "    u = cp.Variable(n)\n",
    "    v = cp.Variable(n)\n",
    "    obj = cp.Maximize(cp.sum(u) - tau * cp.sum(v))\n",
    "    constr = [H @ beta + Z @ u - v == np.ones(m),\n",
    "              u >= 0,\n",
    "              v >= 0,\n",
    "              beta >= 0]\n",
    "    prob = cp.Problem(obj, constr)\n",
    "\n",
    "    # Solve the problem\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "\n",
    "    if prob.status == 'optimal':\n",
    "        # Return the optimal value\n",
    "        return -prob.value\n",
    "    else:\n",
    "        # Return infinity if the problem is infeasible\n",
    "        print('WARNING: Inner loop not optimized')\n",
    "        return np.inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27e25a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The inner loop optimization can also be carried out by `CVXR`.\n",
    "This code snippet is shorter than easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd425732",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy import log, Maximize, Problem, Variable\n",
    "\n",
    "def innerloop_cvxr(b, y=None, X=None, Z=None, tau=None):\n",
    "    n, p = X.shape\n",
    "    m, k = Z.shape\n",
    "\n",
    "    # Generate moment condition\n",
    "    H = MomentMatrix(y, X, Z, b)\n",
    "\n",
    "    # Initialize the CVXPY problem\n",
    "    p = cp.Variable(n)\n",
    "    obj = cp.Maximize(cp.sum(log(p)))\n",
    "    constr = [cp.sum(p) == 1,\n",
    "              p >= 0,\n",
    "              p <= 1,\n",
    "              H @ p >= -tau,\n",
    "              H @ p <= tau]\n",
    "    prob = cp.Problem(obj, constr)\n",
    "\n",
    "    # Solve the problem\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "\n",
    "    if prob.status == 'optimal':\n",
    "        # Return the optimal value\n",
    "        return -prob.value\n",
    "    else:\n",
    "        # Return infinity if the problem is infeasible\n",
    "        print('WARNING: Inner loop not optimized')\n",
    "        return np.inf\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
