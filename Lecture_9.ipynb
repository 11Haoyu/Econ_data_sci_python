{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction-Oriented Algorithms\n",
    "\n",
    "\n",
    "In this lecture, we introduce supervised learning methods that induces\n",
    "data-driven interaction of the covariates.\n",
    "The interaction makes the covariates much more flexible\n",
    "to capture the subtle feature in the data.\n",
    "However, insufficient theoretical understanding is shed little light on these methods due to\n",
    "the complex nature, so they are often viewed by theorists as \"black-boxes\" methods.\n",
    "In real applications, when the machines are carefully tuned, they\n",
    "can achieve surprisingly superior performance.\n",
    "@gu2018empirical showcase a horse race of a myriad of methods,\n",
    "and the general message is that interaction helps with forecast in\n",
    "the financial market.\n",
    "In the meantime, industry insiders are pondering whether these methods are \"alchemy\"\n",
    "which fall short of scientific standard. Caution must be exercised when\n",
    "we apply these methods in empirical economic analysis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Regression Tree and Bagging\n",
    "\n",
    "We consider supervised learning setting in which we use $x$ to predict $y$.\n",
    "It can be done by traditional nonparametric methods such as kernel regression.\n",
    "*Regression tree* [@breiman1984classification] is an alternative to kernel regression. Regression tree  recursively partitions the\n",
    "the space of the regressors. The algorithm is easy to describe: each time a covariate is split into two dummies, and the splitting criterion is aggressive reduction of the SSR. In the formulate of the SSR, the fitted value is computed as the average of $y_i$'s in a partition.\n",
    "\n",
    "The tuning parameter is the depth of the tree, which is referred to the number of splits.\n",
    "Given a dataset $d$ and the depth of the tree, the fitted regression tree $\\hat{r}(d)$ is completely determined by the data.\n",
    "\n",
    "The problem of the regression tree is its instability. For data generated\n",
    "from the same data generating process (DGP), the  covariate chosen to be split and the splitting point can\n",
    "vary widely and they heavily influence the path of the partition.\n",
    "\n",
    "*Bootstrap averaging*, or *bagging* for short, was introduced to reduce the\n",
    "variance of the regression tree [@breiman1996bagging]. Bagging grows a regression tree for\n",
    "each bootstrap sample, and then do a simple average. Let $d^{\\star b}$\n",
    "be the $b$-th bootstrap sample of the original data $d$, and then the\n",
    "bagging estimator is defined as\n",
    "\n",
    "$$\n",
    "\\hat{r}_{\\mathrm{bagging}} = B^{-1} \\sum_{b=1}^B \\hat{r}( d^{\\star b} ).\n",
    "$$\n",
    "\n",
    "Bagging is an example of the *ensemble learning*.\n",
    "\n",
    "@inoue2008useful is an early application of bagging in time series forecast.\n",
    "@hirano2017forecasting provide a theoretical perspective on the risk reduction of bagging.\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "*Random forest* [@breiman2001random] shakes up the regressors by randomly sampling $m$ out of the total $p$ covarites before *each split of a tree*.\n",
    "The tuning parameters in random forest\n",
    "is the tree depth and $m$. Due to the \"de-correlation\" in sampling the regressors, it in\n",
    "general performs better than bagging in prediction exercises.\n",
    "\n",
    "Below is a very simple real data example of random forest using the\n",
    "Boston Housing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'randomForest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrandomForest\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestRegressor\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_boston\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'randomForest'"
     ]
    }
   ],
   "source": [
    "from randomForest import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Set a random seed\n",
    "np.random.seed(101)\n",
    "\n",
    "# Create a training sample with 300 observations\n",
    "train = np.random.choice(range(boston.data.shape[0]), size=300, replace=False)\n",
    "\n",
    "# Fit a random forest regression model to the training data\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=101)\n",
    "rf.fit(boston.data[train], boston.target[train])\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.bar(range(boston.data.shape[1]), rf.feature_importances_)\n",
    "plt.xticks(range(boston.data.shape[1]), boston.feature_names, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the simplicity of the algorithm, the consistency of random forest is not proved\n",
    "until @scornet2015consistency, and\n",
    "inferential theory was first established by\n",
    "@wager2018estimation  in the context of treatment effect estimation.\n",
    "@athey2019generalized generalizes CART to local maximum likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Example**: Random forest for Survey of Professional Forecasters in `data_example/SPF_RF.R` from @cheng2020survey. The script uses `caret` framework.\n",
    "\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "Bagging and random forest almost always use equal weight on each generated tree\n",
    " for the ensemble.\n",
    "Instead, *tree boosting* takes a distinctive scheme to determine the ensemble weights.\n",
    "It is a deterministic approach that does not resample the original data.\n",
    "\n",
    "1. Use the original data $d^0=(x_i,y_i)$ to grow a shallow tree $\\hat{r}^{0}(d^0)$. Save the prediction $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ where\n",
    "$\\alpha\\in [0,1]$ is a shrinkage tuning parameter. Save\n",
    "the residual $e_i^{0} = y_i - f^0_i$. Set $m=1$.\n",
    "2. In the $m$-th iteration, use the data $d^m = (x_i,e_i^{m-1})$ to grow a shallow tree $\\hat{r}^{m}(d^m)$. Save the prediction $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$. Save\n",
    "the residual $e_i^{m} = y_i - f^m_i$. Update $m = m+1$.\n",
    "3. Repeat Step 2 until $m > M$.\n",
    "\n",
    "In this boosting algorithm there are three tuning parameters: the tree depth,  the shrinkage level $\\alpha$, and the number of iterations $M$.\n",
    "The algorithm can be sensitive to all the three tuning parameters.\n",
    "When a model is tuned well, it often performs remarkably.\n",
    "For example, the script `Beijing_housing_gbm.R` achieves much higher out-of-sample $R^2$ than OLS, reported in @lin2020. This script implements boosting via the package `gbm`, which stands for \"Gradient Boosting Machine.\"\n",
    "\n",
    "\n",
    "\n",
    "There are many variants of boosting algorithms. For example, $L_2$-boosting, componentwise boosting, and AdaBoosting, etc. Statisticians view boosting as a gradient descent algorithm to reduce the risk. The fitted\n",
    "tree in each iteration is the deepest descent direction, while the shrinkage tames the fitting to avoid proceeding too aggressively.\n",
    "\n",
    "\n",
    "* @shi2016econometric proposes a greedy algorithm in similar spirit to boosting for moment selection in GMM.\n",
    "* @phillips2019boosting uses $L_2$-boosting as a boosted version of the Hodrick-Prescott filter.\n",
    "* @shi2019forward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "A neural network is the workhorse behind Alpha-Go and self-driven cars.\n",
    "However, from a statistician's point of view it is just a particular type of nonlinear models.\n",
    "Figure 1 illustrates a one-layer neural network, but in general there can be several layers.\n",
    "The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & =  w_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & =  g^{(k)} ( z_l^{(k)}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)}  = x_j$  is the input,  $z_l^{(k)}$ is the $k$-th hidden layer, \n",
    "and all the $w$s are coefficients to be estimated.\n",
    "The above formulation shows that  $z_l^{(k)}$ usually takes a linear form,\n",
    "while the *activation function* $g(\\cdot)$ can be an identity function or a simple nonlinear function.\n",
    "Popular choices of the activation function are sigmoid ($1/(1+\\exp(-x))$) and rectified linear unit (ReLu, $z\\cdot 1\\{x\\geq 0\\}$), etc.\n",
    "\n",
    "\n",
    "![A Single Layer Feedforward Neural Network (from Wiki)](graph/Colored_neural_network.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "A user has several decisions to make when fitting a neural network:\n",
    "besides the activation function,\n",
    "the tuning parameters are the number of hidden layers and the number of nodes in each layer.\n",
    "Many free parameters are generated from the multiple layer and multiple nodes,\n",
    "and in estimation often regularization methods are employed to penalize\n",
    "the $l_1$ and/or $l_2$ norms, which requires extra tuning parameters.\n",
    "`data_example/Keras_ANN.R` gives an example of a neural network\n",
    "with two hidden layers, each has 64 nodes, and the ReLu activation function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Due to the nonlinear nature of the neural networks, theoretical understanding about its behavior is still scant. One of the early contributions came from econometrician: @hornik1989multilayer\n",
    "(Theorem 2.2) show that a single hidden layer neural network, given enough many nodes, is a *universal approximator* for any\n",
    "measurable function.\n",
    "\n",
    "\n",
    "After setting up a neural network, the free parameters must be determined by\n",
    "numerical optimization. The nonlinear complex structure makes the optimization\n",
    "very challenging and the global optimizer is beyond guarantee.\n",
    "In particular, when the sample size is big, the de facto optimization algorithm\n",
    "is the stochastic gradient descent.\n",
    "\n",
    "Thanks to computational\n",
    "scientists, Google's `tensorflow` is a popular backend of\n",
    "neural network estimation, and `keras` is the deep learning modeling language.\n",
    "Their relationship is similar to `Mosek` and `CVXR`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "In optimization we update the $D$-dimensional parameter\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_{k} + a_k p_k,\n",
    "$$\n",
    "\n",
    "where $a_k \\in \\mathbb{R}$ is the step length and $p_k\\in \\mathbb{R}^D$ is a vector\n",
    "of directions. Use a Talyor expansion,\n",
    "\n",
    "$$\n",
    "f(\\beta_{k+1}) = f(\\beta_k + a_k p_k ) \\approx f(\\beta_k) + a_k \\nabla f(\\beta_k) p_k,\n",
    "$$\n",
    "\n",
    "If in each step we want the value of the criterion function\n",
    "$f(x)$ to decrease, we need $\\nabla f(\\beta_k) p_k \\leq 0$.\n",
    "A simple choice is $p_k =-\\nabla f(\\beta_k)$, which is called the deepest decent.\n",
    "Newton's method corresponds to $p_k =- (\\nabla^2 f(\\beta_k))^{-1}  \\nabla f(\\beta_k)$,\n",
    "and BFGS uses a low-rank matrix to approximate $\\nabla^2 f(\\beta_k)$. The linear search is a one-dimensional problem and it can be handled by either exact minimization or backtracking. Details of the descent method is referred to Chapter 9.2--9.5 of @boyd2004convex.\n",
    "\n",
    "When the sample size is huge and the number of parameters is also big,\n",
    "the evaluation of the gradient can be prohibitively expensive.\n",
    "Stochastic gradient descent (SGD) uses a small batch of the sample\n",
    "to evaluate the gradient in each iteration. It can significantly save\n",
    "computational time. It is the *de facto* optimization procedure in complex optimization problems such as\n",
    "training a neural network.\n",
    "\n",
    "However, SGD involves tuning parameters (say, the batch size and the learning rate. Learning rate replaces the step length $a_k$ and becomes a regularization parameter.)\n",
    "that can dramatically affect\n",
    "the outcome, in particular in nonlinear problems.\n",
    "Careful experiments must be carried out before serious implementation.\n",
    "\n",
    "Below is an example of SGD in the PPMLE that we visited in the lecture of optimization, now with sample size 100,000 and\n",
    "the number of parameters 100. SGD is usually much faster than `nlopt`.\n",
    "\n",
    "The new functions are defined with the data explicitly as arguments.\n",
    "Because in SGD each time the log-likelihood function and the gradient are\n",
    "evaluated at a different subsample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpoisson_loglik\u001b[39m(b, y, X):\n\u001b[1;32m      4\u001b[0m     b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masmatrix(b)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def poisson_loglik(b, y, X):\n",
    "    b = np.asmatrix(b)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.mean(-lambda_ + y * np.log(lambda_))\n",
    "    return ell\n",
    "\n",
    "def poisson_loglik_grad(b, y, X):\n",
    "    b = np.asmatrix(b)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.mean((-lambda_ * X) + (y * X), axis=0)\n",
    "    ell_eta = np.asarray(ell).squeeze()\n",
    "    return ell_eta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation \n",
    "\n",
    "In the poisson_loglik() function, we're using the np.exp() function in NumPy to compute the exponential of the matrix product X %*% b in R. We're also using the np.mean() function in NumPy to compute the mean of the negative Poisson log-likelihood.\n",
    "\n",
    "In the poisson_loglik_grad() function, we're computing the gradient of the negative Poisson log-likelihood with respect to the coefficients b. We're using the np.mean() function in NumPy with the axis parameter set to 0 to compute the column means of the gradient. We're also using the np.asarray() and squeeze() functions in NumPy to convert the gradient matrix to a one-dimensional NumPy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoCV results:\n",
      "lambda.min:  0.0012741951349030628\n",
      "MSE:  3.495033563685301\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(898)\n",
    "nn = 100000\n",
    "K = 100\n",
    "X = np.column_stack([np.ones(nn), np.random.rand(nn, K-1)])\n",
    "b0 = np.repeat(1, K) / K\n",
    "y = np.random.poisson(np.exp(np.dot(X, b0)))\n",
    "\n",
    "b_init = np.random.rand(K)\n",
    "b_init = 2 * b_init / sum(b_init)\n",
    "\n",
    "n = len(y)\n",
    "test_ind = np.random.choice(np.arange(n), size=int(0.2*n), replace=False)\n",
    "\n",
    "y_test = y[test_ind]\n",
    "X_test = X[test_ind, :]\n",
    "y_train = y[np.setdiff1d(np.arange(n), test_ind)]\n",
    "X_train = X[np.setdiff1d(np.arange(n), test_ind), :]\n",
    "\n",
    "lasso_cv = LassoCV(cv=10)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "b_lasso_cv = lasso_cv.coef_\n",
    "y_pred = np.dot(X_test, b_lasso_cv)\n",
    "mse_lasso_cv = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"LassoCV results:\")\n",
    "print(\"lambda.min: \", lasso_cv.alpha_)\n",
    "print(\"MSE: \", mse_lasso_cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "1.The code imports the necessary modules: numpy for numerical computing, LassoCV from sklearn.linear_model for Lasso regularization with cross-validation, and mean_squared_error from sklearn.metrics to compute the mean squared error.\n",
    "\n",
    "2.A random seed is set to make the results reproducible.\n",
    "\n",
    "3.The code sets the number of observations nn and the number of predictors K.\n",
    "\n",
    "4.The code generates a design matrix X with nn rows and K columns, where the first column is all ones (for the intercept) and the remaining columns are filled with random values between 0 and 1. The true coefficients b0 are set to have equal weights of 1/K, and the response vector y is generated from a Poisson distribution with a mean of exp(Xb0).\n",
    "\n",
    "5.The code splits the data into training and testing sets by randomly selecting 20% of the observations for the test set.\n",
    "\n",
    "6.The code initializes the coefficients b_init to random values between 0 and 1 and normalizes them so that they sum to 2 (for stability). The training and testing data are separated into X_train, y_train, X_test, and y_test.\n",
    "\n",
    "7.The code fits a LassoCV model with 10-fold cross-validation on the training set using the LassoCV function from scikit-learn. The alpha_ attribute of the resulting model gives the value of the regularization parameter that minimizes the cross-validation error, which is printed to the console. The coefficients b_lasso_cv are also stored.\n",
    "\n",
    "8.The code computes the mean squared error on the test set by multiplying the test design matrix X_test by the estimated coefficients b_lasso_cv and computing the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# optimization parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# sgd depends on\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# * eta: the learning rate\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# upgrade numpy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# !pip install --upgrade numpy==1.24.2\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlopt\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# optimization parameters\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# sgd depends on\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# * eta: the learning rate\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# * epoch: the averaging small batch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# * the initial value\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlopt\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.7.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlopt\\nlopt.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Import the low-level C/C++ module\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __package__ \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _nlopt\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_nlopt\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# optimization parameters\n",
    "# sgd depends on\n",
    "# * eta: the learning rate\n",
    "# * epoch: the averaging small batch\n",
    "# * the initial value\n",
    "# install nlopt\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install nlopt\n",
    "# upgrade numpy\n",
    "# !pip install --upgrade numpy==1.24.2\n",
    "\n",
    "import numpy as np\n",
    "import nlopt\n",
    "import time\n",
    "\n",
    "# optimization parameters\n",
    "# sgd depends on\n",
    "# * eta: the learning rate\n",
    "# * epoch: the averaging small batch\n",
    "# * the initial value\n",
    "\n",
    "max_iter = 5000\n",
    "min_iter = 20\n",
    "eta = 0.01\n",
    "K = 10  # define K\n",
    "epoch = round(100 * np.sqrt(K))\n",
    "\n",
    "b_init = np.zeros(K)  # define b_init\n",
    "b_old = b_init\n",
    "\n",
    "pts0 = time.time()\n",
    "\n",
    "for i in range(1, max_iter+1):\n",
    "    loglik_old, grad_old = poisson_loglik(b_old, None, y_train, X_train)\n",
    "    i_sample = np.random.choice(np.arange(len(y_train)), epoch, replace=True)\n",
    "    b_new = b_old - eta * grad_old[i_sample]\n",
    "    loglik_new, _ = poisson_loglik(b_new, None, y_test, X_test)\n",
    "    b_old = b_new  # update\n",
    "\n",
    "    criterion = loglik_old - loglik_new\n",
    "\n",
    "    if criterion < 0.0001 and i >= min_iter:\n",
    "        break\n",
    "print(\"point estimate =\", b_new, \", log_lik = \", loglik_new, \"\\n\")\n",
    "pts1 = time.time() - pts0\n",
    "print(pts1)\n",
    "\n",
    "# Nelder-Mead method is too slow for this dataset\n",
    "# thus we only sgd with NLoptr\n",
    "opts = {\n",
    "    'algorithm': 'NLOPT_LD_SLSQP',\n",
    "    'xtol_rel': 1.0e-7,\n",
    "    'maxeval': 5000\n",
    "}\n",
    "\n",
    "pts0 = time.time()\n",
    "res_BFGS = nlopt.opt(nlopt.LD_SLSQP, len(b_init))  # fix length of parameter vector\n",
    "res_BFGS.set_lower_bounds([-np.inf] * len(b_init))\n",
    "res_BFGS.set_min_objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "* Efron and Hastie: Chapter 8, 17 and 18.\n",
    "\n",
    "## Quotation\n",
    "\n",
    "<!-- \"The world is yours, as well as ours, but in the last analysis, it is yours. You young people, full of vigor and vitality, are in the bloom of life, like the sun at eight or nine in the morning. Our hope is placed on you.\" -->\n",
    "\n",
    "<!-- ---Mao Zedong, Talk at a meeting with Chinese students and trainees in Moscow (November 17, 1957). -->\n",
    "\n",
    "## References\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
